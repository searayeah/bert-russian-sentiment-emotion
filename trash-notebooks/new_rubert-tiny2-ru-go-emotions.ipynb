{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "import pandas as pd\n",
    "from transformers import (\n",
    "    BertForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "from huggingface_hub import notebook_login\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_config = {\n",
    "    \"project\": \"VKR\",\n",
    "    \"config\": {\n",
    "        \"dataset\": \"seara/ru-go-emotions\",\n",
    "        \"num_labels\": 28,\n",
    "        \"labels\": {\n",
    "            0: \"admiration\",\n",
    "            1: \"amusement\",\n",
    "            2: \"anger\",\n",
    "            3: \"annoyance\",\n",
    "            4: \"approval\",\n",
    "            5: \"caring\",\n",
    "            6: \"confusion\",\n",
    "            7: \"curiosity\",\n",
    "            8: \"desire\",\n",
    "            9: \"disappointment\",\n",
    "            10: \"disapproval\",\n",
    "            11: \"disgust\",\n",
    "            12: \"embarrassment\",\n",
    "            13: \"excitement\",\n",
    "            14: \"fear\",\n",
    "            15: \"gratitude\",\n",
    "            16: \"grief\",\n",
    "            17: \"joy\",\n",
    "            18: \"love\",\n",
    "            19: \"nervousness\",\n",
    "            20: \"optimism\",\n",
    "            21: \"pride\",\n",
    "            22: \"realization\",\n",
    "            23: \"relief\",\n",
    "            24: \"remorse\",\n",
    "            25: \"sadness\",\n",
    "            26: \"surprise\",\n",
    "            27: \"neutral\",\n",
    "        },\n",
    "        \"model\": \"cointegrated/rubert-tiny2\",\n",
    "        \"tokenizer\": \"cointegrated/rubert-tiny2\",\n",
    "        \"problem_type\": \"multi_label_classification\",\n",
    "        \"batch_size\": 64,\n",
    "        \"epochs\": 50,\n",
    "        \"lr\": 1e-5,\n",
    "    },\n",
    "    \"name\": \"rubert-tiny2-ru-go-emotions\",\n",
    "}\n",
    "config = full_config[\"config\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize_labels(labels, num_labels):\n",
    "    return [int(i in labels) for i in range(num_labels)]\n",
    "\n",
    "\n",
    "def label2id(class_labels):\n",
    "    label2id = {label: i for i, label in enumerate(class_labels)}\n",
    "    id2label = {i: label for label, i in label2id.items()}\n",
    "    return label2id, id2label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_aucs(y_true, y_pred, num_labels):\n",
    "    return [roc_auc_score(y_true[:, i], y_pred[:, i]) for i in range(num_labels)]\n",
    "\n",
    "\n",
    "def calculate_f1_score(y_true, y_pred, average, num_labels):\n",
    "    return [\n",
    "        f1_score(y_true[:, i], y_pred[:, i] > 0.5, average=average)\n",
    "        for i in range(num_labels)\n",
    "    ]\n",
    "\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, num_labels):\n",
    "    auc_rocs = calculate_aucs(y_true, y_pred, num_labels)\n",
    "\n",
    "    f1_scores_micro = calculate_f1_score(y_true, y_pred, \"micro\", num_labels)\n",
    "\n",
    "    f1_scores_macro = calculate_f1_score(y_true, y_pred, \"macro\", num_labels)\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        [auc_rocs, f1_scores_micro, f1_scores_macro],\n",
    "        columns=config[\"labels\"].values(),\n",
    "        index=[\"AUC ROC\", \"F1 micro\", \"F1 macro\"],\n",
    "    )\n",
    "    df[\"mean\"] = df.mean(axis=1)\n",
    "    # df[\"mean(emotions)\"] = df.drop(\"no emotion\", axis=1).mean(axis=1)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, dataloader):\n",
    "    with torch.no_grad():\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        for batch in dataloader:\n",
    "            batch = batch.to(model.device)\n",
    "            output = model(**batch)\n",
    "            y_true.append(batch.labels)\n",
    "            y_pred.append(torch.softmax(output.logits, -1))\n",
    "    return torch.cat(y_true).cpu().numpy(), torch.cat(y_pred).cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataloader, optimizer, epochs, test_dataloader):\n",
    "    tq = tqdm(range(epochs))\n",
    "\n",
    "    for epoch in tq:\n",
    "        model.train()\n",
    "        for i, batch in enumerate(train_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            batch = batch.to(model.device)\n",
    "            output = model(**batch)\n",
    "            loss = output.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # if epoch % 5 == 0:\n",
    "        model.eval()\n",
    "\n",
    "        y_true, y_pred = predict(model, train_dataloader)\n",
    "        train_auc = np.mean(calculate_aucs(y_true, y_pred, config[\"num_labels\"]))\n",
    "\n",
    "        y_true, y_pred = predict(model, test_dataloader)\n",
    "        test_auc = np.mean(calculate_aucs(y_true, y_pred, config[\"num_labels\"]))\n",
    "\n",
    "        tq.set_description(f\"loss: {loss.item():4.4f}, AUC: {test_auc:4.4f}\")\n",
    "        wandb.log(\n",
    "            {\"train_auc\": train_auc, \"test_auc\": test_auc, \"train_loss\": loss.item()}\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration seara--ru-go-emotions-010f1c10233a04e9\n",
      "Found cached dataset parquet (/home/seara/.cache/huggingface/datasets/seara___parquet/seara--ru-go-emotions-010f1c10233a04e9/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5925f29d626c4f7d92762e394f127cb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(config[\"dataset\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(config[\"tokenizer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/seara/.cache/huggingface/datasets/seara___parquet/seara--ru-go-emotions-010f1c10233a04e9/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-6a398f7cc851dfd3.arrow\n",
      "Loading cached processed dataset at /home/seara/.cache/huggingface/datasets/seara___parquet/seara--ru-go-emotions-010f1c10233a04e9/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-f9ddef66c5d9538a.arrow\n",
      "Loading cached processed dataset at /home/seara/.cache/huggingface/datasets/seara___parquet/seara--ru-go-emotions-010f1c10233a04e9/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-cfc1b6bfe6f1a825.arrow\n",
      "Loading cached processed dataset at /home/seara/.cache/huggingface/datasets/seara___parquet/seara--ru-go-emotions-010f1c10233a04e9/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-ec812005e89d8824.arrow\n",
      "Loading cached processed dataset at /home/seara/.cache/huggingface/datasets/seara___parquet/seara--ru-go-emotions-010f1c10233a04e9/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-c3778358278953df.arrow\n",
      "Loading cached processed dataset at /home/seara/.cache/huggingface/datasets/seara___parquet/seara--ru-go-emotions-010f1c10233a04e9/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-9c6f71ecf7a63c57.arrow\n"
     ]
    }
   ],
   "source": [
    "config[\"label2id\"], config[\"id2label\"] = label2id(config[\"labels\"].values())\n",
    "processed_dataset = dataset.map(\n",
    "    lambda x: tokenizer(x[\"text\"], truncation=True), batched=True\n",
    ").map(\n",
    "    lambda x: {\n",
    "        \"label\": [\n",
    "            float(y) for y in binarize_labels(x[\"labels\"], config[\"num_labels\"])\n",
    "        ]\n",
    "    },\n",
    "    batched=False,\n",
    "    remove_columns=[\"text\", \"labels\", \"id\",\"ru_text\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cointegrated/rubert-tiny2 were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    config[\"model\"],\n",
    "    num_labels=config[\"num_labels\"],\n",
    "    problem_type=config[\"problem_type\"],\n",
    "    label2id=config[\"label2id\"],\n",
    "    id2label=config[\"id2label\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    concatenate_datasets([processed_dataset[\"train\"], processed_dataset[\"validation\"]]),\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    drop_last=False,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    processed_dataset[\"test\"],\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    drop_last=False,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=data_collator,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=config[\"lr\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mseara\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/seara/Desktop/Github/vkr/new_era/models/wandb/run-20230410_114615-36pr1w4t</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/seara/VKR/runs/36pr1w4t\" target=\"_blank\">rubert-tiny2-ru-go-emotions</a></strong> to <a href=\"https://wandb.ai/seara/VKR\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/seara/VKR/runs/36pr1w4t?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fbae8508e50>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = f\"{os.getcwd()}/{full_config['name']}.ipynb\"\n",
    "wandb.login()\n",
    "wandb.init(**full_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01105eedc989470788a7b65408272d21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "model.cuda()\n",
    "train(model, train_dataloader, optimizer, config[\"epochs\"], test_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>admiration</th>\n",
       "      <th>amusement</th>\n",
       "      <th>anger</th>\n",
       "      <th>annoyance</th>\n",
       "      <th>approval</th>\n",
       "      <th>caring</th>\n",
       "      <th>confusion</th>\n",
       "      <th>curiosity</th>\n",
       "      <th>desire</th>\n",
       "      <th>disappointment</th>\n",
       "      <th>...</th>\n",
       "      <th>nervousness</th>\n",
       "      <th>optimism</th>\n",
       "      <th>pride</th>\n",
       "      <th>realization</th>\n",
       "      <th>relief</th>\n",
       "      <th>remorse</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>neutral</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AUC ROC</th>\n",
       "      <td>0.9102</td>\n",
       "      <td>0.9579</td>\n",
       "      <td>0.8627</td>\n",
       "      <td>0.7612</td>\n",
       "      <td>0.7321</td>\n",
       "      <td>0.7984</td>\n",
       "      <td>0.8912</td>\n",
       "      <td>0.8858</td>\n",
       "      <td>0.8444</td>\n",
       "      <td>0.7569</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7929</td>\n",
       "      <td>0.8556</td>\n",
       "      <td>0.8580</td>\n",
       "      <td>0.6598</td>\n",
       "      <td>0.9064</td>\n",
       "      <td>0.9909</td>\n",
       "      <td>0.8728</td>\n",
       "      <td>0.8660</td>\n",
       "      <td>0.8012</td>\n",
       "      <td>0.8573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 micro</th>\n",
       "      <td>0.9355</td>\n",
       "      <td>0.9792</td>\n",
       "      <td>0.9630</td>\n",
       "      <td>0.9396</td>\n",
       "      <td>0.9315</td>\n",
       "      <td>0.9751</td>\n",
       "      <td>0.9718</td>\n",
       "      <td>0.9488</td>\n",
       "      <td>0.9865</td>\n",
       "      <td>0.9720</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9958</td>\n",
       "      <td>0.9720</td>\n",
       "      <td>0.9971</td>\n",
       "      <td>0.9746</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>0.9932</td>\n",
       "      <td>0.9764</td>\n",
       "      <td>0.9753</td>\n",
       "      <td>0.7481</td>\n",
       "      <td>0.9667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 macro</th>\n",
       "      <td>0.7970</td>\n",
       "      <td>0.8848</td>\n",
       "      <td>0.6850</td>\n",
       "      <td>0.5804</td>\n",
       "      <td>0.6324</td>\n",
       "      <td>0.6545</td>\n",
       "      <td>0.6403</td>\n",
       "      <td>0.6934</td>\n",
       "      <td>0.6846</td>\n",
       "      <td>0.5458</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4989</td>\n",
       "      <td>0.7411</td>\n",
       "      <td>0.4993</td>\n",
       "      <td>0.5512</td>\n",
       "      <td>0.4995</td>\n",
       "      <td>0.8428</td>\n",
       "      <td>0.7420</td>\n",
       "      <td>0.7191</td>\n",
       "      <td>0.7109</td>\n",
       "      <td>0.6821</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          admiration  amusement   anger  annoyance  approval  caring  \\\n",
       "AUC ROC       0.9102     0.9579  0.8627     0.7612    0.7321  0.7984   \n",
       "F1 micro      0.9355     0.9792  0.9630     0.9396    0.9315  0.9751   \n",
       "F1 macro      0.7970     0.8848  0.6850     0.5804    0.6324  0.6545   \n",
       "\n",
       "          confusion  curiosity  desire  disappointment  ...  nervousness  \\\n",
       "AUC ROC      0.8912     0.8858  0.8444          0.7569  ...       0.7929   \n",
       "F1 micro     0.9718     0.9488  0.9865          0.9720  ...       0.9958   \n",
       "F1 macro     0.6403     0.6934  0.6846          0.5458  ...       0.4989   \n",
       "\n",
       "          optimism   pride  realization  relief  remorse  sadness  surprise  \\\n",
       "AUC ROC     0.8556  0.8580       0.6598  0.9064   0.9909   0.8728    0.8660   \n",
       "F1 micro    0.9720  0.9971       0.9746  0.9980   0.9932   0.9764    0.9753   \n",
       "F1 macro    0.7411  0.4993       0.5512  0.4995   0.8428   0.7420    0.7191   \n",
       "\n",
       "          neutral    mean  \n",
       "AUC ROC    0.8012  0.8573  \n",
       "F1 micro   0.7481  0.9667  \n",
       "F1 macro   0.7109  0.6821  \n",
       "\n",
       "[3 rows x 29 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_metrics(*predict(model, test_dataloader), config[\"num_labels\"]).round(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook_login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e7be4a139e64e89b31de10ffbf63103",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/117M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5006e18958b45ca9e27f9c3636cfe6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 1 LFS files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/seara/rubert-tiny2-ru-go-emotions/commit/b6625e16f22a96889e8ca513bc268a4ae4278cf4', commit_message='Upload tokenizer', commit_description='', oid='b6625e16f22a96889e8ca513bc268a4ae4278cf4', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(full_config[\"name\"])\n",
    "tokenizer.push_to_hub(full_config[\"name\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>test_auc</td><td>▁▁▂▃▄▅▅▆▇▇▇▇████████████████████████████</td></tr><tr><td>train_auc</td><td>▁▁▂▂▃▄▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇███████████████</td></tr><tr><td>train_loss</td><td>▇▅▆▆▅▆▅▅▃▂▄█▇▄▂▃▃▃▂▃▄▂▁▃▂▃▂▂▄▁▁▆▃▄▁▃▃▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test_auc</td><td>0.85726</td></tr><tr><td>train_auc</td><td>0.96611</td></tr><tr><td>train_loss</td><td>0.01727</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">rubert-tiny2-ru-go-emotions</strong>: <a href=\"https://wandb.ai/seara/VKR/runs/36pr1w4t\" target=\"_blank\">https://wandb.ai/seara/VKR/runs/36pr1w4t</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230410_114615-36pr1w4t/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
