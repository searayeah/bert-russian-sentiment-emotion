{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {},
 "cells": [
  {
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import wandb\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import notebook_login\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer, BertForSequenceClassification, DataCollatorWithPadding"
   ],
   "cell_type": "code",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "source": [
    "full_config = {\n",
    "    \"project\": \"VKR\",\n",
    "    \"config\": {\n",
    "        \"dataset\": \"cedr\",\n",
    "        \"num_labels\": 6,\n",
    "        \"labels\": {\n",
    "            0: \"no emotion\",\n",
    "            1: \"joy\",\n",
    "            2: \"sadness\",\n",
    "            3: \"surprise\",\n",
    "            4: \"fear\",\n",
    "            5: \"anger\",\n",
    "        },\n",
    "        \"model\": \"DeepPavlov/rubert-base-cased\",\n",
    "        \"tokenizer\": \"DeepPavlov/rubert-base-cased\",\n",
    "        \"problem_type\": \"multi_label_classification\",\n",
    "        \"batch_size\": 64,\n",
    "        \"epochs\": 60,\n",
    "        \"lr\": 1e-5,\n",
    "    },\n",
    "    \"name\": \"rubert-tiny2-cedr\",\n",
    "}\n",
    "config = full_config[\"config\"]"
   ],
   "cell_type": "code",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "source": [
    "def binarize_labels(labels, num_labels):\n",
    "    return [int(len(labels) == 0)] + [int(i in labels) for i in range(num_labels)]\n",
    "\n",
    "\n",
    "def label2id(class_labels):\n",
    "    label2id = {label: i for i, label in enumerate(class_labels)}\n",
    "    id2label = {i: label for label, i in label2id.items()}\n",
    "    return label2id, id2label"
   ],
   "cell_type": "code",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "source": [
    "def calculate_aucs(y_true, y_pred, num_labels):\n",
    "    return [roc_auc_score(y_true[:, i], y_pred[:, i]) for i in range(num_labels)]\n",
    "\n",
    "\n",
    "def calculate_f1_score(y_true, y_pred, average, num_labels):\n",
    "    return [f1_score(y_true[:, i], y_pred[:, i] > 0.5, average=average) for i in range(num_labels)]\n",
    "\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, num_labels):\n",
    "    auc_rocs = calculate_aucs(y_true, y_pred, num_labels)\n",
    "\n",
    "    f1_scores_micro = calculate_f1_score(y_true, y_pred, \"micro\", num_labels)\n",
    "\n",
    "    f1_scores_macro = calculate_f1_score(y_true, y_pred, \"macro\", num_labels)\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        [auc_rocs, f1_scores_micro, f1_scores_macro],\n",
    "        columns=config[\"labels\"].values(),\n",
    "        index=[\"AUC ROC\", \"F1 micro\", \"F1 macro\"],\n",
    "    )\n",
    "    df[\"mean\"] = df.mean(axis=1)\n",
    "    df[\"mean(emotions)\"] = df.drop(\"no emotion\", axis=1).mean(axis=1)\n",
    "    return df"
   ],
   "cell_type": "code",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "source": [
    "def predict(model, dataloader):\n",
    "    with torch.no_grad():\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        for batch in dataloader:\n",
    "            batch = batch.to(model.device)\n",
    "            output = model(**batch)\n",
    "            y_true.append(batch.labels)\n",
    "            y_pred.append(torch.softmax(output.logits, -1))\n",
    "    return torch.cat(y_true).cpu().numpy(), torch.cat(y_pred).cpu().numpy()"
   ],
   "cell_type": "code",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "source": [
    "def train(model, train_dataloader, optimizer, epochs, test_dataloader):\n",
    "    tq = tqdm(range(epochs))\n",
    "\n",
    "    for epoch in tq:\n",
    "        model.train()\n",
    "        for i, batch in enumerate(train_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            batch = batch.to(model.device)\n",
    "            output = model(**batch)\n",
    "            loss = output.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # if epoch % 5 == 0:\n",
    "        model.eval()\n",
    "\n",
    "        y_true, y_pred = predict(model, train_dataloader)\n",
    "        train_auc = np.mean(calculate_aucs(y_true, y_pred, config[\"num_labels\"]))\n",
    "\n",
    "        y_true, y_pred = predict(model, test_dataloader)\n",
    "        test_auc = np.mean(calculate_aucs(y_true, y_pred, config[\"num_labels\"]))\n",
    "\n",
    "        tq.set_description(f\"loss: {loss.item():4.4f}, AUC: {test_auc:4.4f}\")\n",
    "        wandb.log({\"train_auc\": train_auc, \"test_auc\": test_auc, \"train_loss\": loss.item()})"
   ],
   "cell_type": "code",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "source": [
    "dataset = load_dataset(config[\"dataset\"])"
   ],
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "No config specified, defaulting to: cedr/main\n",
      "Found cached dataset cedr (/home/seara/.cache/huggingface/datasets/cedr/main/0.1.1/117570489cbabbdf8de619bd31918a1cd680a7f286b89d04af340d0691dc2d66)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a5f6919813e483cadac058f0f4a752a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {}
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(config[\"tokenizer\"])"
   ],
   "cell_type": "code",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "source": [
    "config[\"label2id\"], config[\"id2label\"] = label2id(config[\"labels\"].values())\n",
    "processed_dataset = dataset.map(lambda x: tokenizer(x[\"text\"], truncation=True), batched=True).map(\n",
    "    lambda x: {\"label\": [float(y) for y in binarize_labels(x[\"labels\"], config[\"num_labels\"] - 1)]},\n",
    "    batched=False,\n",
    "    remove_columns=[\"text\", \"labels\", \"source\"],\n",
    ")"
   ],
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Loading cached processed dataset at /home/seara/.cache/huggingface/datasets/cedr/main/0.1.1/117570489cbabbdf8de619bd31918a1cd680a7f286b89d04af340d0691dc2d66/cache-13dea0cb312c0820.arrow\n",
      "Loading cached processed dataset at /home/seara/.cache/huggingface/datasets/cedr/main/0.1.1/117570489cbabbdf8de619bd31918a1cd680a7f286b89d04af340d0691dc2d66/cache-988743ea8596b197.arrow\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d7d87911fff43a0b1f31ced79018df9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7528 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f620ff5cde844e898b06a4171326dbfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1882 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {}
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    config[\"model\"],\n",
    "    num_labels=config[\"num_labels\"],\n",
    "    problem_type=config[\"problem_type\"],\n",
    "    label2id=config[\"label2id\"],\n",
    "    id2label=config[\"id2label\"],\n",
    ")"
   ],
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    processed_dataset[\"train\"],\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    drop_last=False,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    processed_dataset[\"test\"],\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    drop_last=False,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ],
   "cell_type": "code",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "source": [
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=config[\"lr\"])"
   ],
   "cell_type": "code",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "source": [
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"/home/seara/Desktop/Github/vkr/new_era/rubert-tiny2-cedr.ipynb\"\n",
    "wandb.login()\n",
    "wandb.init(**full_config)"
   ],
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mseara\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "wandb version 0.14.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/seara/Desktop/Github/vkr/new_era/wandb/run-20230408_164427-15c5r4xr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/seara/VKR/runs/15c5r4xr\" target=\"_blank\">rubert-tiny2-cedr</a></strong> to <a href=\"https://wandb.ai/seara/VKR\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/seara/VKR/runs/15c5r4xr?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f4b1ef7fc70>"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "source": [
    "model.cuda()\n",
    "train(model, train_dataloader, optimizer, config[\"epochs\"], test_dataloader)"
   ],
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "874ae59a272d434e84ff7a2b9eea740f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 352.00 MiB (GPU 0; 5.78 GiB total capacity; 3.35 GiB already allocated; 335.81 MiB free; 4.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model\u001b[39m.\u001b[39mcuda()\n\u001b[0;32m----> 2\u001b[0m train(model, train_dataloader, optimizer, config[\u001b[39m\"\u001b[39;49m\u001b[39mepochs\u001b[39;49m\u001b[39m\"\u001b[39;49m], test_dataloader)\n",
      "Cell \u001b[0;32mIn[6], line 12\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_dataloader, optimizer, epochs, test_dataloader)\u001b[0m\n\u001b[1;32m     10\u001b[0m     loss \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mloss\n\u001b[1;32m     11\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m---> 12\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     14\u001b[0m \u001b[39m# if epoch % 5 == 0:\u001b[39;00m\n\u001b[1;32m     15\u001b[0m model\u001b[39m.\u001b[39meval()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/envs/DS/lib/python3.9/site-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    283\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/envs/DS/lib/python3.9/site-packages/torch/optim/optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 33\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     34\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/envs/DS/lib/python3.9/site-packages/torch/optim/adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    130\u001b[0m     beta1, beta2 \u001b[39m=\u001b[39m group[\u001b[39m'\u001b[39m\u001b[39mbetas\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    132\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(\n\u001b[1;32m    133\u001b[0m         group,\n\u001b[1;32m    134\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    139\u001b[0m         state_steps)\n\u001b[0;32m--> 141\u001b[0m     adam(\n\u001b[1;32m    142\u001b[0m         params_with_grad,\n\u001b[1;32m    143\u001b[0m         grads,\n\u001b[1;32m    144\u001b[0m         exp_avgs,\n\u001b[1;32m    145\u001b[0m         exp_avg_sqs,\n\u001b[1;32m    146\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    147\u001b[0m         state_steps,\n\u001b[1;32m    148\u001b[0m         amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    149\u001b[0m         beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    150\u001b[0m         beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    151\u001b[0m         lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    152\u001b[0m         weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    153\u001b[0m         eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    154\u001b[0m         maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    155\u001b[0m         foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    156\u001b[0m         capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    157\u001b[0m         differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    158\u001b[0m         fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    159\u001b[0m         grad_scale\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mgrad_scale\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    160\u001b[0m         found_inf\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mfound_inf\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/envs/DS/lib/python3.9/site-packages/torch/optim/adam.py:281\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 281\u001b[0m func(params,\n\u001b[1;32m    282\u001b[0m      grads,\n\u001b[1;32m    283\u001b[0m      exp_avgs,\n\u001b[1;32m    284\u001b[0m      exp_avg_sqs,\n\u001b[1;32m    285\u001b[0m      max_exp_avg_sqs,\n\u001b[1;32m    286\u001b[0m      state_steps,\n\u001b[1;32m    287\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    288\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    289\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    290\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    291\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m    292\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m    293\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[1;32m    294\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[1;32m    295\u001b[0m      differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[1;32m    296\u001b[0m      grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[1;32m    297\u001b[0m      found_inf\u001b[39m=\u001b[39;49mfound_inf)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/envs/DS/lib/python3.9/site-packages/torch/optim/adam.py:507\u001b[0m, in \u001b[0;36m_multi_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    505\u001b[0m     exp_avg_sq_sqrt \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39m_foreach_sqrt(device_exp_avg_sqs)\n\u001b[1;32m    506\u001b[0m     torch\u001b[39m.\u001b[39m_foreach_div_(exp_avg_sq_sqrt, bias_correction2_sqrt)\n\u001b[0;32m--> 507\u001b[0m     denom \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49m_foreach_add(exp_avg_sq_sqrt, eps)\n\u001b[1;32m    509\u001b[0m torch\u001b[39m.\u001b[39m_foreach_addcdiv_(params_, device_exp_avgs, denom, step_size)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 352.00 MiB (GPU 0; 5.78 GiB total capacity; 3.35 GiB already allocated; 335.81 MiB free; 4.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "source": [
    "calculate_metrics(*predict(model, test_dataloader), config[\"num_labels\"]).round(4)"
   ],
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no emotion</th>\n",
       "      <th>joy</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>fear</th>\n",
       "      <th>anger</th>\n",
       "      <th>mean</th>\n",
       "      <th>mean(emotions)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AUC ROC</th>\n",
       "      <td>0.9261</td>\n",
       "      <td>0.9501</td>\n",
       "      <td>0.9593</td>\n",
       "      <td>0.8951</td>\n",
       "      <td>0.8910</td>\n",
       "      <td>0.7711</td>\n",
       "      <td>0.8988</td>\n",
       "      <td>0.8942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 micro</th>\n",
       "      <td>0.8587</td>\n",
       "      <td>0.9325</td>\n",
       "      <td>0.9277</td>\n",
       "      <td>0.9421</td>\n",
       "      <td>0.9458</td>\n",
       "      <td>0.9118</td>\n",
       "      <td>0.9198</td>\n",
       "      <td>0.9300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 macro</th>\n",
       "      <td>0.8510</td>\n",
       "      <td>0.8854</td>\n",
       "      <td>0.8890</td>\n",
       "      <td>0.8252</td>\n",
       "      <td>0.7862</td>\n",
       "      <td>0.6733</td>\n",
       "      <td>0.8183</td>\n",
       "      <td>0.8129</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          no emotion     joy  sadness  surprise    fear   anger    mean  \\\n",
       "AUC ROC       0.9261  0.9501   0.9593    0.8951  0.8910  0.7711  0.8988   \n",
       "F1 micro      0.8587  0.9325   0.9277    0.9421  0.9458  0.9118  0.9198   \n",
       "F1 macro      0.8510  0.8854   0.8890    0.8252  0.7862  0.6733  0.8183   \n",
       "\n",
       "          mean(emotions)  \n",
       "AUC ROC           0.8942  \n",
       "F1 micro          0.9300  \n",
       "F1 macro          0.8129  "
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "source": [
    "# notebook_login()"
   ],
   "cell_type": "code",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "source": [
    "model.push_to_hub(full_config[\"name\"])\n",
    "tokenizer.push_to_hub(full_config[\"name\"])"
   ],
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b37ac2751964b03bedb5f3bd79c7303",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 1 LFS files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bda26065b6e4344926360a69a69bcb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/117M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/seara/rubert-tiny2-cedr/commit/49d08f3e9f0d5ec37bf4207a19c25f7e858693be', commit_message='Upload tokenizer', commit_description='', oid='49d08f3e9f0d5ec37bf4207a19c25f7e858693be', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "source": [
    "wandb.finish()"
   ],
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>test_auc</td><td>▁▁▃▅▆▆▇▇▇▇▇▇▇█▇████▇█▇███▇███▇█▇▇█▇███▇█</td></tr><tr><td>train_auc</td><td>▁▁▃▄▅▅▆▆▇▇▇▇▇███████████████████████████</td></tr><tr><td>train_loss</td><td>█▇▅▆▅▄▄▄▄▄▃▃▄▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test_auc</td><td>0.89878</td></tr><tr><td>train_auc</td><td>0.99884</td></tr><tr><td>train_loss</td><td>0.00968</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">rubert-tiny2-cedr</strong>: <a href=\"https://wandb.ai/seara/VKR/runs/3hxa1038\" target=\"_blank\">https://wandb.ai/seara/VKR/runs/3hxa1038</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230408_162727-3hxa1038/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {}
    }
   ],
   "execution_count": null
  }
 ]
}
